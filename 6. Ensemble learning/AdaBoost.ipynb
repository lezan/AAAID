{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import copy\n",
    "import sklearn.datasets\n",
    "from sklearn.svm import SVC\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_hastie_10_2()\n",
    "X_train = X[0:8000,:]\n",
    "y_train = y[0:8000]\n",
    "X_test = X[8000:,:]\n",
    "y_test = y[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Implement the AdaBoost ensemble algorithm by completing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostExample:\n",
    "    def __init__(self, weakModel, T):\n",
    "        return None # to be completed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return None # to be completed\n",
    "\n",
    "    def predict(self, X):\n",
    "        return None # to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation you are free to assume:\n",
    "- that the problem is a binary classification problem with labels in $\\{-1, +1\\}$.\n",
    "- that the weakModel can fit a weighted sample set by means of the call `weakModel.fit(X,y,sample_weight=w)` where `w` is a vector of length $|y|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation on the dataset loaded above and using an SVC with a polynomial kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdaBoost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f77419dd39f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweakModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"poly\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0madaboost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweakModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_train_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_test_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdaBoost' is not defined"
     ]
    }
   ],
   "source": [
    "weakModel = SVC(kernel=\"poly\", degree=3)\n",
    "adaboost = AdaBoost(weakModel, 100)\n",
    "y_train_ = c.predict(X_train)\n",
    "y_test_ = c.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate the AdaBoost performances as usual by calculating the classification error. \n",
    "\n",
    "**Note 1**:  \n",
    "since the labels are bound to be in ${+1, -1}$, the classification error can be easily computed as:\n",
    "$$\n",
    "   error(y,y') = \\frac{1}{2} - \\frac{y^T \\times y'}{2N},\n",
    "$$\n",
    "where $N$ is the total number of examples. The formula can be derived noticing that $y^T \\times y'$ calculates the number $N_c$ of examples correctly classified  minus the number $N_{\\bar c}$ of examples incorrectly classified. We have then $y^T \\times y' = N_c - N_{\\bar c}$ and by noticing that $N = N_c + N_{\\bar c}$:\n",
    "$$\n",
    "   N - y^T \\times y' = 2 N_{\\bar c} \\Rightarrow \\frac{N - y^T \\times y'}{2 N} = \\frac{N_{\\bar c}}{N} = error(y,y')\n",
    "$$\n",
    "\n",
    "**Note 2**:\n",
    "do not forget to deepcopy your base model before fitting it to the new data\n",
    "\n",
    "**Note 3**:\n",
    "The SVC model allows specifying weights, but it *does not* work weights are normalized (it works well when the weights are larger). The following class takes normalized weights and denormalize them before passing them to the SVC classifier:\n",
    "\n",
    "```python\n",
    "    class SVC_:\n",
    "        def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "            self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "        def fit(self, X,y,sample_weight=None):\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * len(X)\n",
    "\n",
    "            self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.svc.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/ensemble/weight_boosting.py#L297\n",
    "# https://github.com/jaimeps/adaboost-implementation/blob/master/adaboost.py\n",
    "# https://github.com/xiaoyubai/AdaBoost/blob/master/AdaBoostBinary.py\n",
    "# https://github.com/simsicon/AdaBoostTrees/blob/master/boost.py\n",
    "# https://github.com/prateekbhat91/Decision-Tree/blob/master/ensemble.py\n",
    "# https://github.com/quqixun/MLAlgorithms/blob/master/AdaBoost/src/AdaBoostTree.py\n",
    "# http://rob.schapire.net/papers/explaining-adaboost.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC_:\n",
    "    def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "            self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "    def fit(self, X,y,sample_weight=None):\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = np.multiply(sample_weight, len(X))\n",
    "\n",
    "        self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost(object):\n",
    "\n",
    "    def __init__(self, n_estimators, base_estimator):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator = base_estimator\n",
    "        self.estimators = None\n",
    "        self.alphas = None\n",
    "        self.test_errors = None\n",
    "        self.train_errors = None\n",
    "\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "        self.train_errors = []\n",
    "\n",
    "        train_num = len(X)\n",
    "        \n",
    "        ws_pred_train = np.zeros(train_num)\n",
    "        \n",
    "        weights = np.ones(train_num) / train_num\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            estimator = copy(self.estimator)\n",
    "            \n",
    "            estimator.fit(X, y, sample_weight = weights)\n",
    "\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "            y_predicted = estimator.predict(X)\n",
    "            \n",
    "            if False:\n",
    "                for i in range(len(y_predicted)):\n",
    "                    if y_predicted[i] > 0:\n",
    "                        y_predicted[i] = 1\n",
    "                    else:\n",
    "                        y_predicted[i] = -1\n",
    "\n",
    "            incorrect = [int(i) for i in (y_predicted != y)]\n",
    "            #print(\"incorrect\", incorrect)\n",
    "            error = np.dot(weights, incorrect)\n",
    "            #print(\"error\", error)\n",
    "            alpha = np.log((1 - error) / error) / 2.0\n",
    "            #print(\"alpha\", alpha)\n",
    "            self.alphas.append(alpha)\n",
    "            exp = [np.exp(-1 * alpha * y[i] * y_predicted[i]) for i in range(train_num)]\n",
    "            Z = np.dot(weights, exp)\n",
    "            #print(\"z\", Z)\n",
    "            weights = [w / Z * e for w, e in zip(weights, exp)]\n",
    "            \n",
    "            ws_pred_train += alpha * y_predicted\n",
    "            train_error_rate = sum(np.sign(ws_pred_train) != y) / len(ws_pred_train)\n",
    "            self.train_errors.append(train_error_rate)\n",
    "            \n",
    "            stamp_iteration = 10\n",
    "            if m % stamp_iteration == 0:\n",
    "                print(\"Train error rate: \", train_error_rate)\n",
    "            \n",
    "            #print(\"-------------------------\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        self.test_errors = []\n",
    "        test_num = len(X)\n",
    "        ws_pred_test = np.zeros(test_num)\n",
    "        \n",
    "        predicted = np.zeros(len(X))\n",
    "        i = 0\n",
    "        for clf, alpha in zip(self.estimators, self.alphas):\n",
    "            predicted += alpha * clf.predict(X)\n",
    "            \n",
    "            ws_pred_test += alpha * predicted\n",
    "            test_error_rate = sum(np.sign(ws_pred_test) != y) / len(ws_pred_test)\n",
    "            self.test_errors.append(test_error_rate)\n",
    "            \n",
    "            stamp_iteration = 10\n",
    "            if i % stamp_iteration == 0:\n",
    "                print(\"Test error rate: \", test_error_rate)\n",
    "            i += 1\n",
    "            \n",
    "        return np.sign(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train error rate: ', 0.382625)\n",
      "('Train error rate: ', 0.174125)\n",
      "('Train error rate: ', 0.15525)\n",
      "('Train error rate: ', 0.11375)\n",
      "('Train error rate: ', 0.092625)\n",
      "('Train error rate: ', 0.083125)\n",
      "('Train error rate: ', 0.07325)\n",
      "('Train error rate: ', 0.067875)\n",
      "('Train error rate: ', 0.0585)\n",
      "('Train error rate: ', 0.050375)\n",
      "('Train error rate: ', 0.048375)\n",
      "('Train error rate: ', 0.045125)\n",
      "('Train error rate: ', 0.041375)\n",
      "('Train error rate: ', 0.039375)\n",
      "('Train error rate: ', 0.034125)\n",
      "('Train error rate: ', 0.033375)\n",
      "('Train error rate: ', 0.0295)\n",
      "('Train error rate: ', 0.028625)\n",
      "('Train error rate: ', 0.0245)\n",
      "('Train error rate: ', 0.02425)\n",
      "('Test error rate: ', 0.3835)\n",
      "('Test error rate: ', 0.27975)\n",
      "('Test error rate: ', 0.19375)\n",
      "('Test error rate: ', 0.16175)\n",
      "('Test error rate: ', 0.1455)\n",
      "('Test error rate: ', 0.1315)\n",
      "('Test error rate: ', 0.123)\n",
      "('Test error rate: ', 0.11725)\n",
      "('Test error rate: ', 0.112)\n",
      "('Test error rate: ', 0.1045)\n",
      "('Test error rate: ', 0.09975)\n",
      "('Test error rate: ', 0.09675)\n",
      "('Test error rate: ', 0.0935)\n",
      "('Test error rate: ', 0.08975)\n",
      "('Test error rate: ', 0.0875)\n",
      "('Test error rate: ', 0.08275)\n",
      "('Test error rate: ', 0.08)\n",
      "('Test error rate: ', 0.07775)\n",
      "('Test error rate: ', 0.075)\n",
      "('Test error rate: ', 0.0725)\n",
      "('Accuracy of test set:', 0.94725)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "weak_classifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)\n",
    "# weak_classifier = SVC(kernel=\"poly\", degree=3)\n",
    "# weak_classifier = SVC_(kernel=\"poly\", degree=3)\n",
    "\n",
    "clf_boosted = Adaboost(200, weak_classifier)\n",
    "\n",
    "clf_boosted.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = clf_boosted.predict(X_test, y_test)\n",
    "print(\"Accuracy of test set:\", accuracy_score(y_predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "bdt = AdaBoostClassifier(SVC(kernel=\"poly\", degree=3), n_estimators = 100, algorithm = \"SAMME\") \n",
    "bdt.fit(X_train, y_train)\n",
    "y_pred = bdt.predict(X_test)\n",
    "print(\"Accuracy: \",  accuracy_score(y_test, y_pred))\n",
    "scores = cross_val_score(bdt, X_test, y_test)\n",
    "print(scores.mean())\n",
    "\n",
    "bdt2 = AdaBoostClassifier(n_estimators = 100) \n",
    "bdt2.fit(X_train, y_train)\n",
    "y_pred = bdt2.predict(X_test)\n",
    "print(\"Accuracy: \",  accuracy_score(y_test, y_pred))\n",
    "scores2 = cross_val_score(bdt2, X_test, y_test)\n",
    "print(scores2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a weak learner to be used with the AdaBoost algorithm you just wrote. The weak learner that you will implement shall work as follows:\n",
    "\n",
    "- creates a random linear model by generating the needed weight vector $\\mathbf{w}$ at random; each weight shall be sampled from U(-1,1);\n",
    "- it evaluates the weighted loss $\\epsilon_t$ on the given dataset and flip the linear model if $\\epsilon_t > 0.5$\n",
    "- at prediction time it predicts +1 if $\\mathbf{x} \\cdot \\mathbf{w} > 0$ it predicts -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearModelExample:\n",
    "    def loss(self, y, y_, w):\n",
    "        return None # to be completed\n",
    "        \n",
    "    def fit(self,X,y,sample_weight=None):\n",
    "        return None # to be completed        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        return None # to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn an AdaBoost model using the RandomLinearModel weak learner printing every $K$ iteratins the weighted error and the current error of the ensemble (you are free to choose $K$ so to make your output just frequent enough to let you know what is happening but without flooding the console with messages). Evaluate the training and test error of the final ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomSplit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c1efe486fe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_train_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomSplit' is not defined"
     ]
    }
   ],
   "source": [
    "rs = RandomSplit()\n",
    "a = AdaBoost(rs,10000)\n",
    "a.fit(X_train,y_train)\n",
    "\n",
    "y_train_ = a.predict(X_train)\n",
    "y_test_ = a.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write few paragraphs about what you think about the experiment and about the results you obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearModel:\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        self.classes = dim\n",
    "        self.weights = None\n",
    "\n",
    "    def loss(self, y, y_pred, sample_weight):\n",
    "        return (np.dot(sample_weight, (y_pred - y) ** 2)) / self.classes\n",
    "        \n",
    "    def fit(self, X, y, sample_weight = None):\n",
    "        self.weights = np.random.uniform(-1, 1, self.classes)\n",
    "        y_pred = np.dot(X, self.weights)\n",
    "        loss_value = self.loss(y, y_pred, sample_weight)\n",
    "        if loss_value > 0.5:\n",
    "            self.weights = self.weights * (-1)\n",
    "        \n",
    "        print(\"Loss value: \" + str(loss_value))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights)\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0 ] = -1\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/regression.py\n",
    "# https://www.stat.cmu.edu/~cshalizi/350/lectures/18/lecture-18.pdf\n",
    "# http://www.seas.ucla.edu/~vandenbe/133A/lectures/ls-fitting.pdf\n",
    "# guardare pure la slide di Esposito perché c'è una che si chiama \"Using least squares for classification\"\n",
    "# https://github.com/HarryGogonis/Python-Linear-Least-Squares-Classifier/blob/master/LLS.py\n",
    "# https://www.google.com/search?client=firefox-b&ei=_XSmW6CeKsjClwTaiYOQAQ&q=Using+least+squares+for+classification+python&oq=Using+least+squares+for+classification+python&gs_l=psy-ab.3..33i22i29i30k1.261904.265020.0.265163.7.7.0.0.0.0.297.626.0j2j1.3.0....0...1c.1.64.psy-ab..4.3.624....0.W2dr0PTiWFY\n",
    "# https://github.com/nandhiniramanan5/Classifiers-implemented/blob/master/classalgorithms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 0.26816365943092213\n",
      "('Train error rate: ', 0.489125)\n",
      "Loss value: 0.5274647568104193\n",
      "Loss value: 0.4414358098818612\n",
      "Loss value: 0.43803991480600535\n",
      "Loss value: 0.4123761272274395\n",
      "Loss value: 0.4832130664093028\n",
      "Loss value: 0.4825867486396775\n",
      "Loss value: 0.422024851657356\n",
      "Loss value: 0.5188613229919884\n",
      "Loss value: 0.5760004917927922\n",
      "Loss value: 0.38823530455503147\n",
      "('Train error rate: ', 0.480125)\n",
      "Loss value: 0.3787635599914093\n",
      "Loss value: 0.34499156059612224\n",
      "Loss value: 0.403098448080641\n",
      "Loss value: 0.34241833295494367\n",
      "Loss value: 0.5445295632028184\n",
      "Loss value: 0.6679994827990549\n",
      "Loss value: 0.44040639809876153\n",
      "Loss value: 0.4410782009086508\n",
      "Loss value: 0.41788940852883305\n",
      "Loss value: 0.35099673500591955\n",
      "('Train error rate: ', 0.47825)\n",
      "Loss value: 0.46928031867571063\n",
      "Loss value: 0.43412451091504173\n",
      "Loss value: 0.30480259546642324\n",
      "Loss value: 0.5242675372571759\n",
      "Loss value: 0.48716994679067704\n",
      "Loss value: 0.5838161834702075\n",
      "Loss value: 0.3899075192646517\n",
      "Loss value: 0.2586487383573604\n",
      "Loss value: 0.3146811114200987\n",
      "Loss value: 0.6404007763737145\n",
      "('Train error rate: ', 0.4825)\n",
      "Loss value: 0.4823355012754\n",
      "Loss value: 0.3819045862290965\n",
      "Loss value: 0.3792351351473069\n",
      "Loss value: 0.47987937752427284\n",
      "Loss value: 0.3754279932163658\n",
      "Loss value: 0.4901076421183399\n",
      "Loss value: 0.516610664816681\n",
      "Loss value: 0.4733473581021208\n",
      "Loss value: 0.4909139558265257\n",
      "Loss value: 0.39628646628906583\n",
      "('Train error rate: ', 0.47875)\n",
      "Loss value: 0.40257196330421097\n",
      "Loss value: 0.39470994121628333\n",
      "Loss value: 0.5287293600968852\n",
      "Loss value: 0.40280268818331144\n",
      "Loss value: 0.5131979712732875\n",
      "Loss value: 0.6353959963459627\n",
      "Loss value: 0.40936758670103035\n",
      "Loss value: 0.3740851926662542\n",
      "Loss value: 0.47097099767994416\n",
      "Loss value: 0.5083717495048581\n",
      "('Train error rate: ', 0.478)\n",
      "Loss value: 0.4226756889120281\n",
      "Loss value: 0.4366442858537041\n",
      "Loss value: 0.5581719892210006\n",
      "Loss value: 0.5194169921720755\n",
      "Loss value: 0.3853479329870613\n",
      "Loss value: 0.33518865371878176\n",
      "Loss value: 0.5681720780852904\n",
      "Loss value: 0.4381388502600915\n",
      "Loss value: 0.3181147536246491\n",
      "Loss value: 0.3939025522765339\n",
      "('Train error rate: ', 0.47725)\n",
      "Loss value: 0.47764361665758787\n",
      "Loss value: 0.3901343001700622\n",
      "Loss value: 0.5622816404710156\n",
      "Loss value: 0.4473741514262629\n",
      "Loss value: 0.3767022729680722\n",
      "Loss value: 0.6039936765205625\n",
      "Loss value: 0.4822344718532202\n",
      "Loss value: 0.3364552389484847\n",
      "Loss value: 0.4159255361082864\n",
      "Loss value: 0.3803605889012636\n",
      "('Train error rate: ', 0.472)\n",
      "Loss value: 0.41576887897926246\n",
      "Loss value: 0.39136774526042806\n",
      "Loss value: 0.510354143065619\n",
      "Loss value: 0.3314591713031394\n",
      "Loss value: 0.584247860477154\n",
      "Loss value: 0.43006641461919043\n",
      "Loss value: 0.4670656906621959\n",
      "Loss value: 0.48833019709555125\n",
      "Loss value: 0.45920609652340527\n",
      "Loss value: 0.5238661487931887\n",
      "('Train error rate: ', 0.473375)\n",
      "Loss value: 0.460066243019439\n",
      "Loss value: 0.47635988622826114\n",
      "Loss value: 0.41345729753008376\n",
      "Loss value: 0.34091977925073924\n",
      "Loss value: 0.3428759653165865\n",
      "Loss value: 0.5260502024308235\n",
      "Loss value: 0.638769866204444\n",
      "Loss value: 0.5574164643085694\n",
      "Loss value: 0.5902242405443628\n",
      "Loss value: 0.7170942600367041\n",
      "('Train error rate: ', 0.47475)\n",
      "Loss value: 0.4821089361601607\n",
      "Loss value: 0.39693391515976617\n",
      "Loss value: 0.6110848655686135\n",
      "Loss value: 0.3720422796812718\n",
      "Loss value: 0.42503761436556653\n",
      "Loss value: 0.3626659096582034\n",
      "Loss value: 0.635828263786356\n",
      "Loss value: 0.5624156289656217\n",
      "Loss value: 0.3031609113768453\n",
      "Loss value: 0.20398713042295197\n",
      "('Train error rate: ', 0.4735)\n",
      "Loss value: 0.30535780711700217\n",
      "Loss value: 0.584126800041944\n",
      "Loss value: 0.4459483673938894\n",
      "Loss value: 0.5377208612512139\n",
      "Loss value: 0.30109031994277063\n",
      "Loss value: 0.5849266542763598\n",
      "Loss value: 0.5006427129362799\n",
      "Loss value: 0.28645483241272346\n",
      "Loss value: 0.42691965432398915\n",
      "Loss value: 0.45251986448497866\n",
      "('Train error rate: ', 0.471125)\n",
      "Loss value: 0.5871120513018786\n",
      "Loss value: 0.5374420949135806\n",
      "Loss value: 0.36417230581300764\n",
      "Loss value: 0.3568221737142164\n",
      "Loss value: 0.4149347206565361\n",
      "Loss value: 0.5970943933256935\n",
      "Loss value: 0.4604362718707941\n",
      "Loss value: 0.4935196712270333\n",
      "Loss value: 0.5742944686839041\n",
      "Loss value: 0.5434553019181454\n",
      "('Train error rate: ', 0.465625)\n",
      "Loss value: 0.5944436864162299\n",
      "Loss value: 0.25391627201407785\n",
      "Loss value: 0.3650083831541274\n",
      "Loss value: 0.4282844677693203\n",
      "Loss value: 0.3402720953848068\n",
      "Loss value: 0.6130643790277313\n",
      "Loss value: 0.3157974061589704\n",
      "Loss value: 0.5432923553743562\n",
      "Loss value: 0.44871816136581233\n",
      "Loss value: 0.42945983634740015\n",
      "('Train error rate: ', 0.469125)\n",
      "Loss value: 0.3307603773060293\n",
      "Loss value: 0.39287805798386716\n",
      "Loss value: 0.33690421364623263\n",
      "Loss value: 0.5589095441025539\n",
      "Loss value: 0.350112492881914\n",
      "Loss value: 0.39288710363876855\n",
      "Loss value: 0.3998821389179873\n",
      "Loss value: 0.370879275572798\n",
      "Loss value: 0.42118899366878776\n",
      "Loss value: 0.4925855550187814\n",
      "('Train error rate: ', 0.464125)\n",
      "Loss value: 0.36599371207896353\n",
      "Loss value: 0.5779797809594072\n",
      "Loss value: 0.6315229734834544\n",
      "Loss value: 0.38898252191439503\n",
      "Loss value: 0.3403363509011793\n",
      "Loss value: 0.5182057328839634\n",
      "Loss value: 0.5632619971436753\n",
      "Loss value: 0.41111906913204255\n",
      "Loss value: 0.5751482113628053\n",
      "Loss value: 0.38619509080023856\n",
      "('Train error rate: ', 0.460625)\n",
      "Loss value: 0.6484743822286608\n",
      "Loss value: 0.5281263134044666\n",
      "Loss value: 0.4629092523691676\n",
      "Loss value: 0.4314513915829159\n",
      "Loss value: 0.4824725423853809\n",
      "Loss value: 0.4214447114688715\n",
      "Loss value: 0.3517710269869693\n",
      "Loss value: 0.46116098417278106\n",
      "Loss value: 0.42502347830444737\n",
      "Loss value: 0.6064211565289891\n",
      "('Train error rate: ', 0.459375)\n",
      "Loss value: 0.4102509197350046\n",
      "Loss value: 0.40650621525321257\n",
      "Loss value: 0.48282784233511933\n",
      "Loss value: 0.6927061211882559\n",
      "Loss value: 0.5410330395612807\n",
      "Loss value: 0.5283256200236747\n",
      "Loss value: 0.39441280151253105\n",
      "Loss value: 0.4505530419177253\n",
      "Loss value: 0.586059969318532\n",
      "Loss value: 0.2863064109643215\n",
      "('Train error rate: ', 0.460125)\n",
      "Loss value: 0.4682157157544298\n",
      "Loss value: 0.42858055496596875\n",
      "Loss value: 0.43470063132360987\n",
      "Loss value: 0.5735546799594717\n",
      "Loss value: 0.5654159590297445\n",
      "Loss value: 0.41099871973181923\n",
      "Loss value: 0.4690691466488909\n",
      "Loss value: 0.3946515819080338\n",
      "Loss value: 0.4541516712444647\n",
      "Loss value: 0.5168418585840201\n",
      "('Train error rate: ', 0.451875)\n",
      "Loss value: 0.41324826266436593\n",
      "Loss value: 0.4161356130316041\n",
      "Loss value: 0.40804627648755804\n",
      "Loss value: 0.29372581670776204\n",
      "Loss value: 0.36747680986611925\n",
      "Loss value: 0.24251478028082873\n",
      "Loss value: 0.3932405740743453\n",
      "Loss value: 0.37635828134814425\n",
      "Loss value: 0.29572051242775577\n",
      "Loss value: 0.5202056366600806\n",
      "('Train error rate: ', 0.456875)\n",
      "Loss value: 0.4298769704571804\n",
      "Loss value: 0.5168567274436062\n",
      "Loss value: 0.30812455236429737\n",
      "Loss value: 0.47342558238859533\n",
      "Loss value: 0.46861470318193266\n",
      "Loss value: 0.46131277286946154\n",
      "Loss value: 0.38294667193912835\n",
      "Loss value: 0.34286346419447866\n",
      "Loss value: 0.5717984455593355\n",
      "('Test error rate: ', 0.49175)\n",
      "('Test error rate: ', 0.5045)\n",
      "('Test error rate: ', 0.49275)\n",
      "('Test error rate: ', 0.4895)\n",
      "('Test error rate: ', 0.498)\n",
      "('Test error rate: ', 0.4885)\n",
      "('Test error rate: ', 0.49025)\n",
      "('Test error rate: ', 0.4885)\n",
      "('Test error rate: ', 0.4875)\n",
      "('Test error rate: ', 0.4865)\n",
      "('Test error rate: ', 0.49925)\n",
      "('Test error rate: ', 0.50275)\n",
      "('Test error rate: ', 0.5015)\n",
      "('Test error rate: ', 0.50125)\n",
      "('Test error rate: ', 0.50825)\n",
      "('Test error rate: ', 0.49675)\n",
      "('Test error rate: ', 0.495)\n",
      "('Test error rate: ', 0.49575)\n",
      "('Test error rate: ', 0.4905)\n",
      "('Test error rate: ', 0.497)\n",
      "('Accuracy of test set:', 0.50575)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "weak_classifier2 = RandomLinearModel(X_train.shape[1])\n",
    "# weak_classifier2 = linear_model.LogisticRegression()\n",
    "# weak_classifier2 = linear_model.SGDClassifier(max_iter=1000)\n",
    "# weak_classifier2 = linear_model.LinearRegression()\n",
    "# weak_classifier2 = LinearRegression()\n",
    "\n",
    "clf_boosted2 = Adaboost(200, weak_classifier2)\n",
    "\n",
    "clf_boosted2.fit(X_train, y_train)\n",
    "       \n",
    "y_predicted2 = clf_boosted2.predict(X_test, y_test)\n",
    "print(\"Accuracy of test set:\", accuracy_score(y_predicted2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
